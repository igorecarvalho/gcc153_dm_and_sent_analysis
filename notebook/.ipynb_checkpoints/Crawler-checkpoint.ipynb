{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importação das bibliotecas criadas para normalização dos dados\n",
    "#from nlputils import lexical\n",
    "#from nlputils import morphosyntax\n",
    "#from nlputils import syntax\n",
    "import pandas as pd\n",
    "from nlputils import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexical_normalizer = lexical.Preprocessing()\n",
    "#morphosyntax_normalizer = morphosyntax.Preprocessing()\n",
    "#syntax_normalizer = syntax.Preprocessing()\n",
    "xlsx = '../data/tweets.xlsx'\n",
    "corpora = read_data.Preprocessing(xlsx)\n",
    "tweets = corpora.create_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets['username']\n",
    "del tweets['retweets']\n",
    "del tweets['favorites']\n",
    "del tweets['geo']\n",
    "del tweets['mentions']\n",
    "del tweets['hashtags']\n",
    "del tweets['permalink']\n",
    "tweets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_date(timestamp):\n",
    "    return str(timestamp.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text'] = tweets['text'].apply(corpora.normalize)\n",
    "tweets['date'] = tweets['date'].apply(normalize_date)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets.dropna()\n",
    "tweets = tweets.drop_duplicates(subset='id')\n",
    "tweets = tweets.sort_values('date')\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_groups = {}\n",
    "def group_tweets(tweets, since, until):\n",
    "    for date in tweets.groupby('date'):\n",
    "        if date[0] >= since and date[0] <= until:\n",
    "            date_groups[date[0]] = {'text': [], 'id': [], 'theme': [], 'tweets': []}\n",
    "            #date_groups[date[0]]['text'].extend(date[1]['text'])\n",
    "            date_groups[date[0]]['id'].extend(date[1]['id'])\n",
    "            for sent in date[1]['text']:\n",
    "                date_groups[date[0]]['text'].extend(sent)\n",
    "    return date_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nlputils import lexical\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "normalizer = lexical.Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram_finder(ngram, nfreq, nmost, tagged_sentences):\n",
    "    normalized_sentences = list(itertools.chain.from_iterable(tagged_sentences))\n",
    "    normalized_sentences = normalizer.remove_stopwords(normalized_sentences)\n",
    "    if ngram == 2 :\n",
    "        bifinder = BigramCollocationFinder.from_words(normalized_sentences)\n",
    "        bifinder.apply_freq_filter(nfreq)\n",
    "        return bifinder.nbest(bigram_measures.pmi, nmost)\n",
    "    if ngram == 3:\n",
    "        trifinder = TrigramCollocationFinder.from_words(normalized_sentences)\n",
    "        trifinder.apply_freq_filter(nfreq)\n",
    "        return trifinder.nbest(trigram_measures.pmi, nmost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "date_groups = group_tweets(tweets, '2019-10-01', '2019-10-31')\n",
    "for date in date_groups:\n",
    "    tagged_sentences = date_groups[date]['text']\n",
    "    date_groups[date]['theme'] = n_gram_finder(2, 3, 5, tagged_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(date_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_groups['2019-10-02']['theme'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils import got3 as got"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TwitterCriteria: A collection of search parameters to be used together with TweetManager.\n",
    "\n",
    "- setUsername (str): An optional specific username from a twitter account. Without \"@\".\n",
    "- setSince (str. \"yyyy-mm-dd\"): A lower bound date to restrict search.\n",
    "- setUntil (str. \"yyyy-mm-dd\"): An upper bound date to restrist search.\n",
    "- setQuerySearch (str): A query text to be matched.\n",
    "- setTopTweets (bool): If True only the Top Tweets will be retrieved.\n",
    "- setNear(str): A reference location area from where tweets were generated.\n",
    "- setWithin (str): A distance radius from \"near\" location (e.g. 15mi).\n",
    "- setMaxTweets (int): The maximum number of tweets to be retrieved. If this number is unsetted or lower than 1 all possible tweets will be retrieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet: Model class to give some informations about a specific tweet.\n",
    "\n",
    "- id (str)\n",
    "- permalink (str)\n",
    "- username (str)\n",
    "- text (str)\n",
    "- date (date)\n",
    "- retweets (int)\n",
    "- favorites (int)\n",
    "- mentions (str)\n",
    "- hashtags (str)\n",
    "- geo (str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "DD = datetime.timedelta(days=1)\n",
    "from datetime import datetime\n",
    "def adj_date(str_date):\n",
    "    date = datetime.strptime(str_date, '%Y-%m-%d').date()\n",
    "    return str(date - DD), str(date + DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_tweets(date, theme, maxTweets):\n",
    "    since, until = adj_date(date)\n",
    "    #print(since, ' ', until)\n",
    "    result = []\n",
    "    for word in theme:\n",
    "        #print(word)\n",
    "        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(word).setSince(since).setUntil(until).setMaxTweets(maxTweets)\n",
    "        result.extend(got.manager.TweetManager.getTweets(tweetCriteria))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(date, theme, maxTweets):\n",
    "    tweets = result_tweets(date, theme[0], maxTweets)\n",
    "    result = {'date': [], 'text': [], 'id': []}\n",
    "    for res in tweets:\n",
    "        result['date'].append(res.date)\n",
    "        result['text'].append(res.text)\n",
    "        result['id'].append(res.id)\n",
    "    \n",
    "    result = pd.DataFrame(result)\n",
    "    \n",
    "    name = str(date) + '_' + str(theme[0][1]) + '_' + str(theme[0][1]) + '.csv'\n",
    "    print(name)\n",
    "    result.to_csv(r'../data/tweets_result/' + name, index = None, header=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleta dos tweets por data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(date_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_groups:\n",
    "    date_groups[date]['tweets'] = create_df(date, date_groups[date]['theme'], 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos tweets coletados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depurate_tweets(result_df, theme):\n",
    "    result_df['text'] = result_df['text'].apply(corpora.normalize)\n",
    "    result_df['date'] = result_df['date'].apply(normalize_date)\n",
    "    result_df = result_df.dropna()\n",
    "    result_df = result_df.drop_duplicates(subset='id')\n",
    "    result_df = result_df.sort_values('date')\n",
    "    return result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
